# -*- coding: utf-8 -*-
"""
Created on Wed Nov 13 08:22:43 2019
Notice1:
    label = 1, è¡¨ç¤º entailment
    label = 2, è¡¨ç¤º contradiction
    label = 3, è¡¨ç¤º neutral
Notice2:
    ä½¿ç”¨æ—¶ï¼Œæ³¨æ„ä¿®æ”¹rootè·¯å¾„ä¸ºè‡ªå·±æœ¬åœ°çš„è¯­æ–™è·¯å¾„
    æœ¬è„šæœ¬ä½¿ç”¨äº†snliè¯­æ–™é›†
    å¯¼å…¥çš„è¯­æ–™æ ¼å¼å·²ç»ç»è¿‡é¢„å¤„ç†ï¼Œå…¶æ ¼å¼å¦‚ä¸‹ï¼š
        "[(label,premise,hypothesis),(label,premise,hypothesis),...]"å³ä¸º"[(str1,str2,str3),(str1,str2,str3),...]"
        labelï¼Œpremiseï¼Œhypothesisåˆ†åˆ«ä»£è¡¨æ–‡æœ¬è•´å«å…³ç³»ä¸­çš„æ ‡ç­¾ï¼Œå‡è®¾ï¼Œå‰æ
Notice3:
    ä½¿ç”¨æ—¶ï¼Œå¯ä»¥ä¿®æ”¹devfileä¸trainfileçš„åˆ‡ç‰‡æ•°é‡ã€‚
Notice4ï¼š
    ç¥ç»ç½‘ç»œæ¡†æ¶ï¼š tensorflow 1.14
Notice5:
    éœ€è¦ä½¿ç”¨é™æ€è¯åµŒå…¥
@author: Asus
"""


import pdb
import tensorflow as tf
import numpy as np
import time
root = 'G:\æ•°æ®\snli_1.0\snli_1.0\ä½¿ç”¨pythonè¿›è¡Œé¢„å¤„ç†çš„æ•°æ®'
devfile = 'snli_dev.txt'
testfile = 'snli_test.txt'
trainfile = 'snli_train.txt'
with open(root+'\\'+devfile,'r') as f:
    devfile = eval(f.read())
with open(root+'\\'+trainfile,'r') as f:
    trainfile = eval(f.read())
devfile = trainfile[:500]    
trainfile = trainfile[500:3000]


def predeal(cor):
    '''cor type: [(label,premise,hypothesis),...]'''
    label = []
    premise = []
    hypothesis = []
    for i,j,k in cor:
        if i == 'entailment':
            label.append(1)
        if i == 'contradiction':
            label.append(2)
        if i == 'neutral':
            label.append(3)
        premise.append(j)
        hypothesis.append(k)
    return label,premise,hypothesis

Label_t,Premise_t,Hypothesis_t = predeal(trainfile)
Label_d,Premise_d,Hypothesis_d = predeal(devfile)

Cor_t = list(zip(Premise_t,Hypothesis_t))
Cor_d = list(zip(Premise_d,Hypothesis_d))
def word_info(cor):
    '''word_info: æ”¶é›†è¯æ±‡çš„ä¿¡æ¯
    è¾“å‡º word2ix,word2count,ix2word,vocab_num 
    cor type: [(premise,hypothesis),...]'''
    # Type of cor is zip
    cor_a = [i+' '+j for i,j in cor]
    cor_b = [i.split() for i in cor_a]
    word2ix = {}
    ix2word = {0:'â›”'}
    word2count = {}
    vocab_num = 2
    # å®šä¹‰å‡ ä¸ªå­—å…¸ï¼š
    # word2ix: ä»è¯åˆ°ä¸‹æ ‡çš„å­—å…¸
    # ix2word: ä»ä¸‹æ ‡åˆ°è¯çš„å­—å…¸
    # word2count: ä»è¯åˆ°è¯é¢‘çš„å­—å…¸
    for i in cor_b:
        for j in i:
            if j not in word2ix:
                word2ix[j] = vocab_num
                word2count[j] = 1
                ix2word[vocab_num] = j
                vocab_num += 1
            else:
                word2count[j] += 1
    return word2ix,word2count,ix2word,vocab_num

Word2ix,Word2count,Ix2word,VocabNum = word_info(list(zip(Premise_t+Premise_d,Hypothesis_t+Hypothesis_d)))


def filiter(cor,max_seq_len):
    ''' cor type: [(premise,hypothesis),...]
    
    è¿‡æ»¤æ‰corä¸­é•¿åº¦å¤§äºmax_seq_lençš„å¥å­å¯¹'''
    inde = []
    for i in range(len(cor)):
        len_a = len(cor[i][0].split())
        len_b = len(cor[i][1].split())
        if len_a > max_seq_len or len_b > max_seq_len:
            inde.append(i)
    for j in inde[::-1]:
        cor.pop(j)
    return cor


def line_to_array(sentence,word2ix=Word2ix,max_seq_len = 30):
    '''å°†sentenceçš„å†…å®¹æ ¹æ®wordçš„indexè½¬åŒ–ä¸ºarray'''
    sen_lis = sentence.split()
    mat = np.zeros(max_seq_len)
    for i in range(len(sen_lis)):
        mat[i] = word2ix[sen_lis[i]]
    # è¿™é‡Œæ²¡æœ‰ç”¨åˆ°å¥å­çš„å¼€å§‹ç¬¦ï¼Œè¿™ä¸ªç­‰åˆ°å®é™…å»ºæ¨¡å†åšä¿®æ”¹ï¼Œç°åœ¨è¿˜ä¸çŸ¥é“å¥å­çš„å¼€å§‹ç¬¦æ˜¯å¦æœ‰ä½œç”¨
    return mat


def arraybatch_from_cor(cor,label,word2ix = Word2ix,max_seq_len = 30,batch_size = 100,shuffle=True):
    '''cor type: [(premise,hypothesis),...]
       label type: [label,label,...]
       word2ix type: {'word1':index1,
                      'word2':index2,
                      ...}'''
      # å…ˆè¯•è¯•çœ‹æŠŠæ‰€æœ‰çš„batchç”Ÿæˆä¸€ä¸ªè¿­ä»£å™¨
    if shuffle == True:
        long = len(cor)
        cor_index = list(range(long))
        np.random.shuffle(cor_index)
        cor_copy,label_copy = [],[]
        for i in range(len(cor_index)):
            cor_copy.append(cor[cor_index[i]])
            label_copy.append(label[cor_index[i]])
        cor = cor_copy
        label = label_copy
    res_number = len(cor)%batch_size
    if res_number != 0:
        trac_num = batch_size*(len(cor)//batch_size) 
        cor = cor[:trac_num]
    assert len(cor)%batch_size == 0
    batch_num = len(cor)//batch_size
#    index = 1
    batch_lis = []
    for i in range(batch_num):
        mat_premise = np.zeros(shape = [batch_size,max_seq_len],dtype = np.int32)
        mat_hypothesis = np.zeros(shape = [batch_size,max_seq_len],dtype = np.int32)
        mat_label = np.zeros(shape = [batch_size,3],dtype = np.int32)
        temp_cor = cor[i*batch_size:(i+1)*batch_size]
        label_cor = label[i*batch_size:(i+1)*batch_size]
        for vp in range(len(temp_cor)):
              premise_sentence = temp_cor[vp][0]
              mat_pre = line_to_array(premise_sentence,max_seq_len = max_seq_len)
              mat_premise[vp] = mat_pre
              hypothesis_sentence = temp_cor[vp][1]
              mat_hyp = line_to_array(hypothesis_sentence,max_seq_len = max_seq_len)
              mat_hypothesis[vp] = mat_hyp
              mat_label[vp,label_cor[vp]-1] = 1
        batch_lis.append([mat_label,mat_premise,mat_hypothesis])
    return batch_lis,len(batch_lis)

##  è¦æŠŠBatché‡Œé¢dtypeè°ƒæ•´ä¸ºtf.int32
class ESIM():
    # ç»ˆäºï¼Œåˆ°äº†æœ€ä¸ç¡®å®šçš„ä¸€ç¯å•¦ ğŸ˜„ ğŸ˜„ ğŸ˜„
    # å½“ä½¿ç”¨tensorflowæ—¶ï¼Œå¦‚ä½•å»ºç«‹æ¨¡å‹ï¼Œä¸æ˜¯ä¸€ä¸ªç®€å•çš„é—®é¢˜ï¼Œfor me
    def __init__(self,vocab_num:int,embedding_dim:int,batch_size:int,
                 max_seq_len:int,lstm_hidden_dim:int,dropout:float):
        # åˆå§‹åŒ–çš„é€»è¾‘æ˜¯å®šä¹‰å¿…è¦çš„å˜é‡
        # input.shape = [100,30]
        self.p_input = tf.placeholder(tf.int32,[batch_size,max_seq_len])
        self.h_input = tf.placeholder(tf.int32,[batch_size,max_seq_len])
        self.real_label = tf.placeholder(tf.int32,[batch_size,3])
        self.dropout = tf.placeholder(tf.float32)
        
        self.lstm_hidden_dim = lstm_hidden_dim
        self.max_seq_len = max_seq_len
        
        
        with tf.variable_scope('embedding',reuse = tf.AUTO_REUSE):
            self.embedding_w = tf.Variable(initial_value = tf.random.normal([vocab_num,embedding_dim],0,0.1))
            self.p_embedding = self.droptensor(tf.nn.embedding_lookup(self.embedding_w,self.p_input),self.dropout)
            self.q_embedding = self.droptensor(tf.nn.embedding_lookup(self.embedding_w,self.h_input),self.dropout)
            
        # step1 Input by BiLSTM
        with tf.variable_scope('input_bilstm',reuse = tf.AUTO_REUSE):
            lstm_cell_fw1 = tf.nn.rnn_cell.LSTMCell(num_units = 300,dtype = tf.float32)
            lstm_cell_fw1 = tf.contrib.rnn.DropoutWrapper(cell = lstm_cell_fw1, input_keep_prob = 1-0.2) # 1-hparam.dropout
            lstm_cell_bw1 = tf.nn.rnn_cell.LSTMCell(num_units = 300,dtype = tf.float32)            
            lstm_cell_bw1 = tf.contrib.rnn.DropoutWrapper(cell = lstm_cell_bw1, input_keep_prob = 1-0.2)
            pre_out,pre_hidden= tf.nn.bidirectional_dynamic_rnn(cell_fw = lstm_cell_fw1,
                                    cell_bw = lstm_cell_bw1, inputs = self.p_embedding,dtype = tf.float32)
            a1 = tf.concat([pre_out[0],pre_out[1]],axis = 2)
            
            hyp_out,hyp_hidden = tf.nn.bidirectional_dynamic_rnn(cell_fw = lstm_cell_fw1,
                                    cell_bw = lstm_cell_bw1, inputs = self.q_embedding,dtype = tf.float32)            
            b1 = tf.concat([hyp_out[0],hyp_out[1]],axis = 2)
            
#            self.a1_hidden = tf.concat((self.pre_hidden[0][1],self.pre_hidden[1][1]),axis = -1)
#            self.b1_hidden = tf.concat((self.hyp_hidden[0][1],self.hyp_hidden[1][1]),axis = -1)
            
        # step2 Local Inference Modeling
        with tf.variable_scope('local_infer'):
            infer_pre = tf.split(a1,batch_size,0)
            infer_hyp = tf.split(b1,batch_size,0)
            ## self.infer_pre : [tensor,tensor,...]
            ##                  [shape(1,max_time,embedding_dim)] 
            pre_batch = len(infer_pre)
            attention_batch = [0 for i in range(pre_batch)]
            for i in range(pre_batch):
                pre_sentence = infer_pre[i]
                pre_sentence = tf.squeeze(pre_sentence, axis = 0)
                # pre_sentence.shape = [max_seq_len,lstm_out_dim] (30,400)
#                pre_shape = pre_sentence.shape.as_list()
#                pre_word = tf.split(pre_sentence,pre_shape[0],0)
                
                hyp_sentence = infer_hyp[i]
                hyp_sentence = tf.squeeze(hyp_sentence, axis = 0)
                # pre_sentence.shape = [max_seq_len,lstm_out_dim] (30,400)

#                hyp_shape = hyp_sentence.shape.as_list()
#                hyp_word = tf.split(hyp_sentence,hyp_shape[0],0)
                # =====è®¡ç®—attentionçŸ©é˜µçš„å€¼
                attention_ph = tf.matmul(pre_sentence,tf.transpose(hyp_sentence))
                sum_attn = tf.reduce_mean(attention_ph,axis=1)
                sum_attn = tf.expand_dims(sum_attn,1)
                sum_attn = tf.tile(sum_attn,[1,attention_ph.shape.as_list()[1]])
                attention_ph = tf.math.divide(attention_ph,sum_attn)

                attention_hp = tf.matmul(hyp_sentence,tf.transpose(pre_sentence))
                sum_attn = tf.reduce_mean(attention_hp,axis=1)
                sum_attn = tf.expand_dims(sum_attn,1)
                sum_attn = tf.tile(sum_attn,[1,attention_hp.shape.as_list()[1]])
                attention_hp = tf.math.divide(attention_hp,sum_attn)
                attention_hp = tf.stack(attention_hp)
                
                attention_batch[i] = (attention_ph,attention_hp)
            # è‡³æ­¤ï¼Œæˆ‘ä»¬å¾—åˆ°äº†æ¯ä¸ªbatchçš„ä¸€ä¸ªå­˜å‚¨attentionçš„åˆ—è¡¨(ä¸”æŒ‡æ•°å½’ä¸€åŒ–äº†)
            # ä¸”åˆ—è¡¨çš„å…ƒç´ å°±æ˜¯batchä¸­æ¯ä¸ªå¯¹åº”çš„på’Œhä¸¤å¥è¯æ„æˆçš„attentionçŸ©é˜µ
            # è¯¥çŸ©é˜µçš„i,jå…ƒç´ è¡¨ç¤ºpçš„ç¬¬iä¸ªè¯å’Œqçš„ç¬¬jä¸ªè¯çš„embeddingçš„å†…ç§¯
            
            # ç°åœ¨è®¡ç®—a2ï¼Œb2
            a2_batch = []  # è¯¥åˆ—è¡¨ç”¨æ¥å­˜å‚¨è¯¥æ‰¹æ¬¡æ¯ä¸ªpremiseæ ·æœ¬äº§ç”Ÿçš„a2ï¼Œb2_batchç±»ä¼¼
            b2_batch = []
            for i in range(pre_batch):
                matrix_ph = attention_batch[i][0]
                matrix_hp = attention_batch[i][1]
                
                tem_b =  infer_hyp[i]
                temp_a = tf.matmul(matrix_ph,tem_b)
                sen_a2 = tf.reduce_sum(temp_a,axis = 0)
                
                tem_a =  infer_pre[i]
                temp_b = tf.matmul(matrix_hp,tem_a)
                sen_b2 = tf.reduce_sum(temp_b,axis = 0)
                
                a2_batch.append(sen_a2)
                b2_batch.append(sen_b2)
            
            # ========è‡³æ­¤ï¼Œæˆ‘ä»¬å·²ç»è®¡ç®—å®Œæˆa2,b2ï¼Œæ¥ä¸‹æ¥è¿æ¥[a1,a2,a1-a2,a1âŠ™a2]
            a2 = tf.stack(a2_batch)
            b2 = tf.stack(b2_batch)
            ## self.a2.shape = [batch_size,max_time,embedding_dim]
            ma = tf.concat([a1,a2,tf.abs(a1-a2),tf.math.multiply(a1,a2)],axis = -1)
            mb = tf.concat([b1,b2,tf.abs(b1-b2),tf.math.multiply(b1,b2)],axis = -1)
            
        #==== step3: Inference Composition
        with tf.variable_scope('Inference',reuse=tf.AUTO_REUSE):
            shape_a = ma.shape.as_list()[2]
            shape_b = mb.shape.as_list()[2]

            # imap æ˜¯ç”¨æ¥é™ç»´çš„
            
            w3_a = tf.Variable(tf.random.normal([shape_a,self.lstm_hidden_dim],0,1),dtype = tf.float32)
            w3_b = tf.Variable(tf.random.normal([shape_b,self.lstm_hidden_dim],0,1),dtype = tf.float32)
            ma_f = tf.nn.relu(tf.matmul(ma,w3_a))
            mb_f = tf.nn.relu(tf.matmul(mb,w3_b))
            
            lstm_cell_fw2 = tf.nn.rnn_cell.LSTMCell(num_units = 300,dtype = tf.float32)
            lstm_cell_fw2 = tf.contrib.rnn.DropoutWrapper(cell = lstm_cell_fw2, input_keep_prob = 1-0.2) # 1-hparam.dropout
            lstm_cell_bw2 = tf.nn.rnn_cell.LSTMCell(num_units = 300,dtype = tf.float32)            
            lstm_cell_bw2 = tf.contrib.rnn.DropoutWrapper(cell = lstm_cell_bw2, input_keep_prob = 1-0.2)
            
            
            # å‰åä¸¤ä¸ªçš„rnnå’Œçš„ç¥ç»å…ƒè¦ä¸€è‡´ï¼Œå¦åˆ™å°±ä¸èƒ½å¤Ÿä½¿ç”¨å‰é¢è¾“å‡ºçš„çŠ¶æ€å±‚äº†
            va_out,_ = tf.nn.bidirectional_dynamic_rnn(lstm_cell_fw2,lstm_cell_bw2,
                            ma_f,dtype = tf.float32)
            vb_out,_ = tf.nn.bidirectional_dynamic_rnn(lstm_cell_fw2,lstm_cell_bw2,
                            mb_f,dtype=tf.float32)
            
        with tf.variable_scope('full_connect',reuse=tf.AUTO_REUSE):

            va = tf.concat([va_out[0],va_out[1]],axis=-1)
            vb = tf.concat([vb_out[0],vb_out[1]],axis=-1)
            va_max = tf.reduce_max(va,axis = 1)
            vb_max = tf.reduce_max(vb,axis = 1)
            va_mean = tf.reduce_mean(va, axis = 1)
            vb_mean = tf.reduce_mean(vb, axis = 1)
            self.v = tf.concat([va_max,va_mean,vb_max,vb_mean],axis = -1)
            v_long = self.v.shape.as_list()[-1]
            w4 = tf.Variable(tf.random.normal([v_long,self.lstm_hidden_dim],0,1),dtype = tf.float32)
            b4 = tf.Variable(tf.random.normal([self.lstm_hidden_dim],0,1),dtype = tf.float32)
            mid  = tf.nn.relu(tf.matmul(self.v,w4)+b4)
            
            w5 = tf.Variable(tf.random.normal([self.lstm_hidden_dim,3],0,1),dtype = tf.float32)
            b5 = tf.Variable(tf.random.normal([3],0,1),dtype = tf.float32)
            self.logits = tf.matmul(mid,w5)+b5
            
            
        # =================æ„å»ºæŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        with tf.variable_scope('loss_optimizer',reuse=tf.AUTO_REUSE):
            lossfun = tf.nn.softmax_cross_entropy_with_logits
            loss = lossfun(labels = self.real_label,logits = self.logits)
            self.loss = tf.reduce_mean(loss)
            equal = tf.equal(tf.argmax(self.real_label,1),tf.argmax(self.logits,1))
            self.acc = tf.reduce_sum(tf.cast(equal,tf.float32))
            optimizer = tf.train.AdamOptimizer(0.001,beta1=0.9, beta2=0.999,epsilon=1e-8)
            self.opdate = optimizer.minimize(self.loss)
    def droptensor(self,tensor,keeprob):
        return tf.nn.dropout(tensor,keeprob)
# å‚æ•°åŒº     
EmbeddingDim = 300  
LstmHiddenDim = 300
BatchSize = 32
MaxSeqLen = 50
Dropout = 0.5     
model = ESIM(vocab_num=VocabNum,embedding_dim=EmbeddingDim,batch_size=BatchSize,
                 max_seq_len=MaxSeqLen,lstm_hidden_dim=LstmHiddenDim,dropout=Dropout)
#model_dev = ESIM(infer = True,vocab_num=Vocab_num,embedding_dim=Embedding_dim,batch_size=Batch_size,max_seq_len=30)

sess = tf.Session()
init = tf.global_variables_initializer()
sess.run(init)
print('æ‰¹æ¬¡å¤§å°ï¼š%d'%BatchSize)
Epochs = 5

res = [[] for i in range(Epochs)]
Cor_t = filiter(cor = Cor_t, max_seq_len = MaxSeqLen)
Cor_d = filiter(cor = Cor_d, max_seq_len = MaxSeqLen)
Batch,nt = arraybatch_from_cor(Cor_t,Label_t,batch_size=BatchSize,max_seq_len = MaxSeqLen,shuffle=True)
Batch_d,nd = arraybatch_from_cor(Cor_d,Label_d,batch_size=BatchSize,max_seq_len = MaxSeqLen)
Sample_sum_tr = nt*BatchSize
Sample_sum_dv = nd*BatchSize

#ppd = []
start = time.time()
for epoch in range(Epochs):
    print('ç¬¬%dæ¬¡è®­ç»ƒ'%(epoch+1))
    ##  ç”±äºåŒå‘lstmæœ‰ä¸¤ä¸ªæ–¹å‘ï¼Œå› æ­¤preçš„çŠ¶æ€æ˜¯ç”±åµŒå¥—å…ƒç»„æ„æˆçš„ã€‚
    tr_loss,tr_acc = 0,0 
    te_loss,te_acc = 0,0
    for t,[i,j,k] in enumerate(Batch):
        Feed_dict = {model.p_input:j, model.h_input:k, model.real_label:i, model.dropout:Dropout}
        loss_value,acc,_ = sess.run([model.loss,model.acc,model.opdate],feed_dict = Feed_dict)
        tr_loss += loss_value
        tr_acc += acc
        if t % 4 == 0:
            print('è¿­ä»£æ¬¡æ•°ï¼š%2d,æŸå¤±å€¼ï¼š%.3f, ç²¾ç¡®ç‡ï¼š%.1f%%'%(t,loss_value,acc/BatchSize*100))
    tr_loss,tr_acc = tr_loss /t,tr_acc / Sample_sum_tr    
    for t,[i,j,k] in enumerate(Batch_d):
        dev_dict = {model.p_input:j, model.h_input:k, model.real_label:i, model.dropout:Dropout}
        loss_value2,acc2 = sess.run([model.loss,model.acc],feed_dict = dev_dict)
        te_loss += loss_value2
        te_acc += acc2
    te_loss,te_acc = te_loss /t,te_acc/Sample_sum_dv
    timeuse = time.time() - start
    minutes = int(timeuse // 60)
    seconds = int(timeuse % 60)
    print('epoch = %2d'%(epoch+1))
    print('æ—¶é—´å·²ç”¨: %2dmin,%2dsec'%(minutes,seconds))
    print('è®­ç»ƒé›†, train loss = %.4f, train accuracy = %.1f%%'%(tr_loss,tr_acc*100))
    print('æµ‹è¯•é›†, dev loss = %.4f, dev accuracy = %.1f%%'%(te_loss,te_acc*100))
    
           
